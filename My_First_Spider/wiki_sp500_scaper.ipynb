{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51da20ae19e467cb91708c4d5a84f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=505), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from requests_html import HTMLSession\n",
    "import os\n",
    "import re\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm # progress bar\n",
    "\n",
    "session = HTMLSession()\n",
    "#Then scrape the URLs\n",
    "\n",
    "#### SECTION 1: get the urls to crawl (or data that can build the urls)\n",
    "\n",
    "if not os.path.exists('input/sp500_urls.csv'):\n",
    "\n",
    "    # open page\n",
    "    session = HTMLSession() \n",
    "    url = 'https://en.m.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    r = session.get(url)\n",
    "\n",
    "    # find the links\n",
    "    table = r.html.find('#constituents')[0]  # reduce to the table\n",
    "    table_rows = table.find('tr')[1:]\n",
    "    colinks = []\n",
    "    for row in table_rows:\n",
    "        a_url =   list(row.find('td')[1].absolute_links)[0]    \n",
    "        colinks.append(a_url)\n",
    "\n",
    "    # save list to csv    \n",
    "    if not os.path.exists('input/'):\n",
    "        os.makedirs('input/')\n",
    "    pd.DataFrame(colinks).to_csv('input/sp500_urls.csv',index=False,header=False)\n",
    "    \n",
    "else:\n",
    "    colinks = pd.read_csv('input/sp500_urls.csv',names=['firm']) ['firm'].to_list()\n",
    "#Then download the URLs\n",
    "\n",
    "#### SECTION 2: crawl the urls\n",
    "\n",
    "# for url in colinks[:10]:\n",
    "for url in tqdm(colinks,total=len(colinks)):\n",
    "    \n",
    "    folder = 'wiki_html/'\n",
    "    if not os.path.exists('wiki_html/'):\n",
    "        os.makedirs('wiki_html/')\n",
    "    \n",
    "    name = re.sub( r'\\W+','',     url.split('/')[-1]    )\n",
    "    filepath = folder + name + '.txt'\n",
    "        \n",
    "    if not os.path.exists(filepath):\n",
    "        \n",
    "        try:\n",
    "            r = session.get(url)\n",
    "        except:\n",
    "            print('sad face failure:',url)\n",
    "        else:   \n",
    "            with open(filepath,'w',encoding='utf8') as f:\n",
    "                f.write(r.text)\n",
    "            sleep(2) # be nice to server     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element 'table' class=('wikitable', 'sortable') id='constituents'>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = HTMLSession()\n",
    "#Then scrape the URLs\n",
    "\n",
    "#### SECTION 1: get the urls to crawl (or data that can build the urls)\n",
    "\n",
    "\n",
    "    # open page\n",
    "session = HTMLSession() \n",
    "url = 'https://en.m.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "r = session.get(url)\n",
    "r.html.find('#constituents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
